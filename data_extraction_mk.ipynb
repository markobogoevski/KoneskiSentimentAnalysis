{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regular-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./Results/MK', exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "insured-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_list(var):\n",
    "    flat = [item for sublist in var for item in sublist]\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wireless-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/Koneski_final_finished_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "union-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "stihovi_mkd = df['Stih']\n",
    "pesni_mkd = df['Pesna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operational-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "stihovi_list = stihovi_mkd.to_list()\n",
    "pesni_list = pesni_mkd.to_list()\n",
    "pesni_list = set(pesni_list)\n",
    "pesni_list = (list(pesni_list))\n",
    "pesni_list = [x for x in pesni_list if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "foreign-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('mk_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-orientation",
   "metadata": {},
   "source": [
    "# Stihovi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-saskatchewan",
   "metadata": {},
   "source": [
    "## Word count without lematization and with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "starting-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "stihovi_cleaned = []\n",
    "stihovi_sentences = []\n",
    "ids = []\n",
    "for i, stih in enumerate(stihovi_list):\n",
    "    if type(stih) is not float:\n",
    "        stih = stih.lower()\n",
    "        stih = stih.split()\n",
    "        stih = [re.sub(r'[^\\w\\s]','',word) for word in stih]\n",
    "        stih = [word for word in stih if word.isalnum()]\n",
    "        if len(stih) == 0:\n",
    "            continue\n",
    "        stihovi_sentences.append(' '.join(stih))\n",
    "        stihovi_cleaned.append(stih)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "consistent-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eligible-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'sentence': stihovi_sentences, \n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "contrary-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('./Results/MK/sentences_without_lem_with_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-attitude",
   "metadata": {},
   "source": [
    "### Average words in stih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "stunning-baptist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.416950959488273\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 0\n",
    "for stih in stihovi_cleaned:\n",
    "    number_of_words += len(stih)\n",
    "print(number_of_words/len(stihovi_cleaned))\n",
    "stihovi_cleaned_flat = flat_list(stihovi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "nonprofit-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "stihovi_cleaned_flat = flat_list(stihovi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "forty-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count = Counter(stihovi_cleaned_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "backed-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df = pd.DataFrame(stih_count.most_common())\n",
    "stih_count_df.columns = ['words', 'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "blond-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df.to_csv('./Results/MK/stih_word_count_without_lem_with_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "broadband-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df_10 = stih_count_df[stih_count_df['count'] > 10]\n",
    "stih_count_df_10.to_csv('./Results/MK/stih_word_count_lem_with_stopwords_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "moving-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_stopwords = stih_count_df[stih_count_df['count'] > stih_count_df['count'].quantile(.80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "affecting-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_stopwords.to_csv('./Results/MK/potential_stopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "latter-regular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>да</td>\n",
       "      <td>1237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>на</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>се</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>во</td>\n",
       "      <td>765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>нас</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>би</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>нè</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>зошто</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>еден</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  count\n",
       "0       и   1512\n",
       "1      да   1237\n",
       "2      на   1049\n",
       "3      се   1034\n",
       "4      во    765\n",
       "..    ...    ...\n",
       "65    нас     56\n",
       "66     би     56\n",
       "67     нè     55\n",
       "68  зошто     55\n",
       "69   еден     54\n",
       "\n",
       "[70 rows x 2 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_stopwords.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "special-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_stopwords = potential_stopwords[potential_stopwords['count']>50]['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "accessory-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_stopwords = potential_stopwords.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-looking",
   "metadata": {},
   "source": [
    "## Word count with lematization and removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "possible-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "како било кално –\n",
      "['било', 'кално']\n"
     ]
    }
   ],
   "source": [
    "stih = stihovi_list[2]\n",
    "print(stih)\n",
    "stih = stih.lower()\n",
    "stih = nltk.word_tokenize(stih)\n",
    "stih = [re.sub(r'[^\\w\\s]','',word) for word in stih]\n",
    "stih = [word for word in stih if not word in potential_stopwords]\n",
    "stih = [word for word in stih if word.isalpha()]\n",
    "print(stih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "voluntary-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'за' in potential_stopwords:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "toxic-criminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "stihovi_cleaned = []\n",
    "stihovi_sentences = []\n",
    "ids = []\n",
    "for i, stih in enumerate(stihovi_list):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if type(stih) is not float and type(stih) is not int:\n",
    "        stih = stih.lower()\n",
    "        stih = nltk.word_tokenize(stih)\n",
    "        stih = [re.sub(r'[^\\w\\s]','',word) for word in stih]\n",
    "        stih = [word for word in stih if not word in potential_stopwords]\n",
    "        stih = [word for word in stih if word.isalpha()]\n",
    "        processed_stih = []\n",
    "        for word in stih:\n",
    "            doc = nlp(word)\n",
    "            for each_word in doc:\n",
    "                lemma = each_word.lemma_ \n",
    "            processed_stih.append(lemma)\n",
    "        if len(processed_stih) == 0:\n",
    "            continue\n",
    "        stihovi_sentences.append(' '.join(processed_stih))\n",
    "        stihovi_cleaned.append(processed_stih)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "medical-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "general-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'sentence': stihovi_sentences, \n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "specialized-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('./Results/MK/sentences_with_lem_without_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-massachusetts",
   "metadata": {},
   "source": [
    "### Average words per stih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "thick-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6036406978004116\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 0\n",
    "for stih in stihovi_cleaned:\n",
    "    number_of_words += len(stih)\n",
    "print(number_of_words/len(stihovi_cleaned))\n",
    "stihovi_cleaned_flat = flat_list(stihovi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "requested-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "stihovi_cleaned_flat = flat_list(stihovi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "agricultural-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count = Counter(stihovi_cleaned_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "sticky-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df = pd.DataFrame(stih_count.most_common())\n",
    "stih_count_df.columns = ['words', 'count']\n",
    "stih_count_df.to_csv('./Results/MK/stih_word_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "impaired-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df_10 = stih_count_df[stih_count_df['count'] > 10]\n",
    "stih_count_df_10.to_csv('./Results/MK/stih_word_count_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-reconstruction",
   "metadata": {},
   "source": [
    "## Word count without lematization and removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "mediterranean-discretion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "stihovi_cleaned = []\n",
    "stihovi_sentences = []\n",
    "ids = []\n",
    "for i, stih in enumerate(stihovi_list):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if type(stih) is not float and type(stih) is not int:\n",
    "        stih = stih.lower()\n",
    "        stih = nltk.word_tokenize(stih)\n",
    "        stih = [re.sub(r'[^\\w\\s]','',word) for word in stih]\n",
    "        stih = [word for word in stih if not word in potential_stopwords]\n",
    "        stih = [word for word in stih if word.isalpha()]\n",
    "        processed_stih = []\n",
    "        for word in stih:\n",
    "            processed_stih.append(word)\n",
    "        if len(processed_stih) == 0:\n",
    "            continue\n",
    "        stihovi_sentences.append(' '.join(processed_stih))\n",
    "        stihovi_cleaned.append(processed_stih)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "invalid-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "optional-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'sentence': stihovi_sentences, \n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "presidential-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('./Results/sentences_without_lem_without_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-advocacy",
   "metadata": {},
   "source": [
    "### Average words per stih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "extended-payment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6036406978004116\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 0\n",
    "for stih in stihovi_cleaned:\n",
    "    number_of_words += len(stih)\n",
    "print(number_of_words/len(stihovi_cleaned))\n",
    "stihovi_cleaned_flat = flat_list(stihovi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "based-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "stihovi_cleaned_flat = flat_list(stihovi_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "copyrighted-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count = Counter(stihovi_cleaned_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "found-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df = pd.DataFrame(stih_count.most_common())\n",
    "stih_count_df.columns = ['words', 'count']\n",
    "stih_count_df.to_csv('./Results/MK/stih_word_count_without_lem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "developing-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "stih_count_df_10 = stih_count_df[stih_count_df['count'] > 10]\n",
    "stih_count_df_10.to_csv('./Results/MK/stih_word_count_without_lem_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-parameter",
   "metadata": {},
   "source": [
    "# Pesni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-playback",
   "metadata": {},
   "source": [
    "## Word count with lematization and removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "selective-ownership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pesni_cleaned = []\n",
    "pesni_sentences = []\n",
    "ids = []\n",
    "for i, pesna in enumerate(pesni_list):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if type(pesna) is not float and type(pesna) is not int:\n",
    "        pesna = pesna.lower()\n",
    "        pesna = nltk.word_tokenize(pesna)\n",
    "        pesna = [re.sub(r'[^\\w\\s]','',word) for word in pesna]\n",
    "        pesna = [word for word in pesna if not word in potential_stopwords]\n",
    "        pesna = [word for word in pesna if word.isalpha()]\n",
    "        processed_pesna = []\n",
    "        for word in pesna:\n",
    "            doc = nlp(word)\n",
    "            for each_word in doc:\n",
    "                lemma = each_word.lemma_ \n",
    "            processed_pesna.append(lemma)\n",
    "        if len(processed_pesna) == 0:\n",
    "            continue\n",
    "        pesni_sentences.append(' '.join(processed_pesna))\n",
    "        pesni_cleaned.append(processed_pesna)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "unsigned-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "massive-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'pesna': pesni_sentences, \n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ethical-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('./Results/MK/pesni_lem_without_stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-compatibility",
   "metadata": {},
   "source": [
    "### Average words per pesna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "simple-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.03256704980843\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 0\n",
    "pesna_len = []\n",
    "for pesna in pesni_cleaned:\n",
    "    number_of_words += len(pesna)\n",
    "    pesna_len.append(len(pesna))\n",
    "print(number_of_words/len(pesni_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "dried-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter = Counter(pesna_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "brutal-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df = pd.DataFrame.from_dict(pesna_len_counter, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "manual-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df = pesna_len_counter_df.rename(columns={'index':'words_in_pesna', 0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "unlimited-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df.to_csv('./Results/MK/word_count_in_pesna_lem_without_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-certificate",
   "metadata": {},
   "source": [
    "## Word count without lematization and removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ruled-magnitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pesni_cleaned = []\n",
    "pesni_sentences = []\n",
    "ids = []\n",
    "for i, pesna in enumerate(pesni_list):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if type(pesna) is not float and type(pesna) is not int:\n",
    "        pesna = pesna.lower()\n",
    "        pesna = nltk.word_tokenize(pesna)\n",
    "        pesna = [re.sub(r'[^\\w\\s]','',word) for word in pesna]\n",
    "        pesna = [word for word in pesna if not word in potential_stopwords]\n",
    "        pesna = [word for word in pesna if word.isalpha()]\n",
    "        processed_pesna = []\n",
    "        for word in pesna:\n",
    "            processed_pesna.append(word)\n",
    "        if len(processed_pesna) == 0:\n",
    "            continue\n",
    "        pesni_sentences.append(' '.join(processed_pesna))\n",
    "        pesni_cleaned.append(processed_pesna)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "opening-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "periodic-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'pesna': pesni_sentences, \n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "identical-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('./Results/MK/pesni_without_lem_without_stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-principle",
   "metadata": {},
   "source": [
    "### Average words per pesna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "emerging-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.03256704980843\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 0\n",
    "pesna_len = []\n",
    "for pesna in pesni_cleaned:\n",
    "    number_of_words += len(pesna)\n",
    "    pesna_len.append(len(pesna))\n",
    "print(number_of_words/len(pesni_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "architectural-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter = Counter(pesna_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "binary-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df = pd.DataFrame.from_dict(pesna_len_counter, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "executive-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df = pesna_len_counter_df.rename(columns={'index':'words_in_pesna', 0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "steady-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df.to_csv('./Results/MK/word_count_in_pesna_no_lem_without_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-cargo",
   "metadata": {},
   "source": [
    "## Word count without lematization and without removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fourth-north",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pesni_cleaned = []\n",
    "pesni_sentences = []\n",
    "ids = []\n",
    "for i, pesna in enumerate(pesni_list):\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    if type(pesna) is not float and type(pesna) is not int:\n",
    "        pesna = pesna.lower()\n",
    "        pesna = nltk.word_tokenize(pesna)\n",
    "        pesna = [re.sub(r'[^\\w\\s]','',word) for word in pesna]\n",
    "        pesna = [word for word in pesna if word.isalpha()]\n",
    "        processed_pesna = []\n",
    "        for word in pesna:\n",
    "            processed_pesna.append(word)\n",
    "        if len(processed_pesna) == 0:\n",
    "            continue\n",
    "        pesni_sentences.append(' '.join(processed_pesna))\n",
    "        pesni_cleaned.append(processed_pesna)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "sexual-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "hawaiian-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'pesna': pesni_sentences, \n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "expanded-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('./Results/MK/pesni_without_lem_with_stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-archive",
   "metadata": {},
   "source": [
    "### Average words per pesna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "mental-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.2183908045977\n"
     ]
    }
   ],
   "source": [
    "number_of_words = 0\n",
    "pesna_len = []\n",
    "for pesna in pesni_cleaned:\n",
    "    number_of_words += len(pesna)\n",
    "    pesna_len.append(len(pesna))\n",
    "print(number_of_words/len(pesni_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "outside-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter = Counter(pesna_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "increasing-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df = pd.DataFrame.from_dict(pesna_len_counter, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "inside-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df = pesna_len_counter_df.rename(columns={'index':'words_in_pesna', 0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "residential-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "pesna_len_counter_df.to_csv('./Results/MK/word_count_in_pesna_no_lem_with_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-store",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gentle-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pd.read_csv('./Results/MK/sentences_with_lem_without_stop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "placed-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = sentences['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "weighted-spending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    лаеле пците\n",
       "1                     било кален\n",
       "2                    сето наваса\n",
       "3             темничиштево пален\n",
       "4                   крваво време\n",
       "                  ...           \n",
       "9224                поништи ужас\n",
       "9225                       човек\n",
       "9226                  може сфати\n",
       "9227     проклета слабост зборне\n",
       "9228    поживинченост ако молчиш\n",
       "Name: sentence, Length: 9229, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stone-religious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'било кален'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opposite-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"пес за живот\")\n",
    "print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "    print(\"doc.ents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "appreciated-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "entities = []\n",
    "ids = []\n",
    "for i, sentence in enumerate(sentences_list):\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        words.append(ent.text)\n",
    "        entities.append(ent.label_)\n",
    "        ids.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fixed-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "godina_od_list = df.iloc[ids, 1]\n",
    "godina_do_list = df.iloc[ids, 2]\n",
    "ime_na_pesni = df.iloc[ids, :]['Pesna_Ime']\n",
    "ime_na_zbirka = df.iloc[ids, :]['Zbirka']\n",
    "ime_na_podzbirka = df.iloc[ids, :]['Podzbirka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "european-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_entities_mk = pd.DataFrame({'word': words, \n",
    "                             'entity': entities,\n",
    "                             'year_from': godina_od_list, \n",
    "                             'year_to': godina_do_list, \n",
    "                             'song_name':ime_na_pesni, \n",
    "                             'zbirka_name':ime_na_zbirka, \n",
    "                             'podzbirka_name':ime_na_podzbirka})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "seventh-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entity</th>\n",
       "      <th>year_from</th>\n",
       "      <th>year_to</th>\n",
       "      <th>song_name</th>\n",
       "      <th>zbirka_name</th>\n",
       "      <th>podzbirka_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>темничиштево</td>\n",
       "      <td>NORP</td>\n",
       "      <td>1941</td>\n",
       "      <td>1945</td>\n",
       "      <td>ПЕСНА ЗА ЖИВОТОТ</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ииии</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>1941</td>\n",
       "      <td>1945</td>\n",
       "      <td>ПЕСНА ЗА ЖИВОТОТ</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>горок</td>\n",
       "      <td>GPE</td>\n",
       "      <td>1941</td>\n",
       "      <td>1945</td>\n",
       "      <td>КАМБАНИ</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ноќта</td>\n",
       "      <td>TIME</td>\n",
       "      <td>1941</td>\n",
       "      <td>1945</td>\n",
       "      <td>НОЌНА ПЕСНА</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>опијани</td>\n",
       "      <td>GPE</td>\n",
       "      <td>1941</td>\n",
       "      <td>1945</td>\n",
       "      <td>НОЌНА ПЕСНА</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "      <td>ОД СТАРИОТ НОТЕС (1941‡1945)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9138</th>\n",
       "      <td>секавица</td>\n",
       "      <td>GPE</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>МРТВА ПРИРОДА</td>\n",
       "      <td>ЦРН ОВЕН (1993)</td>\n",
       "      <td>ОTPОР</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9149</th>\n",
       "      <td>сврдле</td>\n",
       "      <td>GPE</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>ЏИЏЕ</td>\n",
       "      <td>ЦРН ОВЕН (1993)</td>\n",
       "      <td>ОTPОР</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9176</th>\n",
       "      <td>еднаш</td>\n",
       "      <td>TIME</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>ОБРОК</td>\n",
       "      <td>ЦРН ОВЕН (1993)</td>\n",
       "      <td>ОTPОР</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9214</th>\n",
       "      <td>допрва</td>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>АНТИЧКА ТРАГЕДИЈА</td>\n",
       "      <td>ЦРН ОВЕН (1993)</td>\n",
       "      <td>ОTPОР</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9219</th>\n",
       "      <td>атанас</td>\n",
       "      <td>NORP</td>\n",
       "      <td>1993</td>\n",
       "      <td>1993</td>\n",
       "      <td>УРНА</td>\n",
       "      <td>ЦРН ОВЕН (1993)</td>\n",
       "      <td>ОTPОР</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>934 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word   entity  year_from  year_to          song_name  \\\n",
       "3     темничиштево     NORP       1941     1945   ПЕСНА ЗА ЖИВОТОТ   \n",
       "9             ииии   PERSON       1941     1945   ПЕСНА ЗА ЖИВОТОТ   \n",
       "27           горок      GPE       1941     1945            КАМБАНИ   \n",
       "30           ноќта     TIME       1941     1945        НОЌНА ПЕСНА   \n",
       "33         опијани      GPE       1941     1945        НОЌНА ПЕСНА   \n",
       "...            ...      ...        ...      ...                ...   \n",
       "9138      секавица      GPE       1993     1993      МРТВА ПРИРОДА   \n",
       "9149        сврдле      GPE       1993     1993               ЏИЏЕ   \n",
       "9176         еднаш     TIME       1993     1993              ОБРОК   \n",
       "9214        допрва  ORDINAL       1993     1993  АНТИЧКА ТРАГЕДИЈА   \n",
       "9219        атанас     NORP       1993     1993               УРНА   \n",
       "\n",
       "                       zbirka_name                podzbirka_name  \n",
       "3     ОД СТАРИОТ НОТЕС (1941‡1945)  ОД СТАРИОТ НОТЕС (1941‡1945)  \n",
       "9     ОД СТАРИОТ НОТЕС (1941‡1945)  ОД СТАРИОТ НОТЕС (1941‡1945)  \n",
       "27    ОД СТАРИОТ НОТЕС (1941‡1945)  ОД СТАРИОТ НОТЕС (1941‡1945)  \n",
       "30    ОД СТАРИОТ НОТЕС (1941‡1945)  ОД СТАРИОТ НОТЕС (1941‡1945)  \n",
       "33    ОД СТАРИОТ НОТЕС (1941‡1945)  ОД СТАРИОТ НОТЕС (1941‡1945)  \n",
       "...                            ...                           ...  \n",
       "9138               ЦРН ОВЕН (1993)                         ОTPОР  \n",
       "9149               ЦРН ОВЕН (1993)                         ОTPОР  \n",
       "9176               ЦРН ОВЕН (1993)                         ОTPОР  \n",
       "9214               ЦРН ОВЕН (1993)                         ОTPОР  \n",
       "9219               ЦРН ОВЕН (1993)                         ОTPОР  \n",
       "\n",
       "[934 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_entities_mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "micro-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_entities_mk.to_csv('./Results/MK/ner.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-tobacco",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
